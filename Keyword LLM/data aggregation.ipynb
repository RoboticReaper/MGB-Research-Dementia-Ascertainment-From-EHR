{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f07896-f277-4424-afda-0e2344925265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import psycopg2\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc379a6-8a4f-493e-b0ff-27661fbddce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stanza pipeline for English with only tokenization (no parsing to keep it efficient)\n",
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=False)\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to tokenize sentences using Stanza with fallback to SpaCy\n",
    "def tokenize_with_fallback(text, max_sentence_length=300):\n",
    "    # First attempt tokenization with Stanza\n",
    "    sentences = stanza_tokenize(text)\n",
    "    \n",
    "    # Check if any sentence exceeds the max length and re-tokenize with SpaCy if needed\n",
    "    final_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > max_sentence_length:\n",
    "            spacy_sentences = spacy_tokenize(sentence)\n",
    "            final_sentences.extend(spacy_sentences)\n",
    "        else:\n",
    "            final_sentences.append(sentence)\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Function to tokenize using Stanza\n",
    "def stanza_tokenize(text):\n",
    "    doc = nlp_stanza(text)\n",
    "    return [sentence.text for sentence in doc.sentences]\n",
    "\n",
    "# Function to tokenize using SpaCy\n",
    "def spacy_tokenize(text):\n",
    "    doc = nlp_spacy(text)\n",
    "    return [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b937ee5-df67-4ec3-8f55-034e7323eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lists of keywords\n",
    "keywords_list1 = {\n",
    "    'alzheimer', 'dementia', 'demented', 'mild cognitive impairment', 'amnesia', 'amnestic', 'frontotemporal', 'neurocognit',\n",
    "    'lewy body', 'aphasia', 'memory', 'Donepezil', 'Aricept', 'Galantamine', 'Reminyl', 'Razadyne', 'Rivastigmine', 'Exelon', \n",
    "    'Memantine', 'Namenda', 'Namzaric', 'Aducanumab', 'Aduhelm', 'Lecanemab', 'Leqembi', 'confus', 'amnesia', 'forget', \n",
    "    'forgot', 'word', 'disorientation', 'attention', 'problem solving', 'executive dysfunction', \n",
    "    'poor safety awareness', 'mood', 'speech', 'neuropsych', 'visuospatial', 'Mini-Cog', 'Mini-Mental State Examination', \n",
    "    'Montreal cognitive assessment', 'clinical dementia rating', 'Saint Louis University Mental Status Exam', 'cogniti', \n",
    "    'memory', 'alert', 'oriented', 'recall', 'attention', 'processing speed', 'verbal fluency', 'encoding', 'visuospatial', \n",
    "    'naming', 'orientation'\n",
    "}\n",
    "\n",
    "keywords_list2 = {'MCI', 'FTD', 'LBD', 'MMSE', 'GPCOG', 'AD8', 'SLUMS', 'MoCA', 'BDS', 'CDR', 'BNT'}\n",
    "\n",
    "# Convert keywords to lowercase for easier searching (except for keywords_list2 as we need exact matching)\n",
    "keywords_list1 = {kw.lower() for kw in keywords_list1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254f74e-5cc9-4630-8348-62a116c870f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the database\n",
    "conn = psycopg2.connect(\n",
    ")\n",
    "\n",
    "# SQL query to retrieve the relevant notes data\n",
    "# exclude radiology reports\n",
    "query = \"\"\"\n",
    "\n",
    "SELECT empi, report_date, notetxt\n",
    "FROM adrd.adrd_study_700_notes_all\n",
    "WHERE notetype!= 'rad' ;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "notes_data = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39c66e-fb21-419b-9cd0-d8e8c73a78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the notes by removing special characters and spaces\n",
    "notes_data['notetxt'] = notes_data['notetxt'].str.replace(r'[\\r\\n]+', ' ', regex=True)\n",
    "notes_data['notetxt'] = notes_data['notetxt'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "notes_data['notetxt'] = notes_data['notetxt'].apply(lambda text: re.sub(r'-', '', text))\n",
    "notes_data['notetxt'] = notes_data['notetxt'].apply(lambda text: re.sub(r'\\(.*?\\)', '', text))\n",
    "notes_data['notetxt'] = notes_data['notetxt'].apply(lambda text: re.sub(r'\\*', '', text))\n",
    "notes_data['notetxt'] = notes_data['notetxt'].apply(lambda text: re.sub(r'(.)\\1{2,}', r'\\1\\1', text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d24d35-43a6-44c7-9c4d-41e5f10a8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1ecc9-c797-4756-a3e0-bfe23cdbb9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for keywords in a sentence (for keywords_list1, we use partial matching)\n",
    "def search_keywords_list1(sentence, keywords):\n",
    "    sentence_lower = sentence.lower()\n",
    "    return any(kw in sentence_lower for kw in keywords)\n",
    "\n",
    "# Function to search for keywords in a tokenized sentence (for keywords_list2, we use exact matching)\n",
    "def search_keywords_list2(tokenized_sentence, keywords):\n",
    "    return any(word in keywords for word in tokenized_sentence)\n",
    "\n",
    "# Function to get unique context indices\n",
    "def get_context_indices(sentences, idx):\n",
    "    context_indices = {idx}  # Include the sentence itself\n",
    "    if idx > 0:  # Add previous sentence index if exists\n",
    "        context_indices.add(idx - 1)\n",
    "    if idx < len(sentences) - 1:  # Add next sentence index if exists\n",
    "        context_indices.add(idx + 1)\n",
    "    return context_indices\n",
    "\n",
    "# Process the notes and extract relevant sentences with unique context indices\n",
    "def extract_relevant_sentences(note, keywords_list1, keywords_list2, seen_sentences):\n",
    "    sentences = tokenize_with_fallback(note, max_sentence_length)  # Using Stanza tokenizer\n",
    "    unique_indices = set()\n",
    "\n",
    "    # Identify relevant sentences and their context indices\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        tokenized_sentence = sentence.split()\n",
    "\n",
    "        if (search_keywords_list1(sentence, keywords_list1) or search_keywords_list2(tokenized_sentence, keywords_list2)) and sentence not in seen_sentences:\n",
    "            seen_sentences.add(sentence)\n",
    "            unique_indices.update(get_context_indices(sentences, idx))\n",
    "\n",
    "    # Retrieve sentences based on unique indices and maintain order\n",
    "    relevant_sentences = [sentences[i] for i in sorted(unique_indices)]\n",
    "    \n",
    "    # Join the relevant sentences into a single string\n",
    "    return \" \".join(relevant_sentences)\n",
    "\n",
    "# Sequential processing function with progress tracking\n",
    "def process_notes_data(notes_data):\n",
    "    # Initialize empty list for filtered texts and counter\n",
    "    filtered_texts = []\n",
    "    print_interval = 5000  # Set the interval for printing progress\n",
    "\n",
    "    for i, note in enumerate(notes_data['notetxt']):\n",
    "        empi - notes_data.loc[i, 'empi']\n",
    "\n",
    "         # Initialize a seen_sentences set for each patient\n",
    "        if empi not in seen_sentences:\n",
    "            seen_sentences[empi] = set()       \n",
    "            \n",
    "        # Extract relevant sentences for each note\n",
    "        filtered_texts.append(extract_relevant_sentences(note, keywords_list1, keywords_list2, seen_sentences[empi]))\n",
    "        \n",
    "        # Print progress every 5000 notes\n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(f\"Processed {i + 1} notes out of {len(notes_data['notetxt'])}\")\n",
    "\n",
    "    notes_data['filtered_text'] = filtered_texts\n",
    "\n",
    "    # Group by patient and report date and concatenate notes\n",
    "    grouped_notes = notes_data.groupby(['empi', 'report_date'])['filtered_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    return grouped_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d2f5a-35d6-407e-afa4-46efb328cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e052e-49c2-4e65-9da6-9088cfb8aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the seen_sentences dictionary to track identified sentences per patient\n",
    "seen_sentences = {}\n",
    "\n",
    "grouped_notes = process_notes_data(notes_data)\n",
    "\n",
    "# Optionally, save the result to a CSV file\n",
    "grouped_notes.to_csv('data/adrd_filtered_patient_notes_stanza.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d08bb-de0d-4573-bd0d-734111ca4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered notes data\n",
    "filtered_notes = pd.read_csv(\"data/adrd_filtered_patient_notes_stanza_v2.csv\")\n",
    "\n",
    "# Filter out rows with no actual clinical notes\n",
    "filtered_notes = filtered_notes[\n",
    "    (filtered_notes['filtered_text'].notna()) &\n",
    "    (filtered_notes['filtered_text'].str.strip() != '') &\n",
    "    (filtered_notes['filtered_text'].str.lower() != 'null')\n",
    "]\n",
    "\n",
    "filtered_notes['filtered_text'] = filtered_notes['filtered_text'].replace('Null', '')\n",
    "\n",
    "# Load other tables assuming they are saved as CSV files from the SQL queries\n",
    "diagnosis = pd.read_csv(\"data/adrd_combined_diagnosis.csv\")  # Result of Diagnosis query\n",
    "medications = pd.read_csv(\"data/adrd_combined_medications.csv\")  # Result of Medications query\n",
    "problem_list = pd.read_csv(\"data/adrd_combined_problems.csv\")  # Result of Problem List query\n",
    "dob_data = pd.read_csv(\"data/adrd_dob.csv\")  # Contains EMPI and date of birth\n",
    "\n",
    "\n",
    "# Rename columns for consistency if necessary\n",
    "filtered_notes.rename(columns={'report_date': 'record_date'}, inplace=True)\n",
    "diagnosis.rename(columns={'diagnosis_date': 'record_date'}, inplace=True)\n",
    "medications.rename(columns={'medication_date': 'record_date'}, inplace=True)\n",
    "problem_list.rename(columns={'diagnosis_date': 'record_date'}, inplace=True)\n",
    "\n",
    "# Convert dates to datetime for consistency\n",
    "dob_data['dob'] = pd.to_datetime(dob_data['dob'])\n",
    "filtered_notes['record_date'] = pd.to_datetime(filtered_notes['record_date'])\n",
    "diagnosis['record_date'] = pd.to_datetime(diagnosis['record_date'])\n",
    "medications['record_date'] = pd.to_datetime(medications['record_date'])\n",
    "problem_list['record_date'] = pd.to_datetime(problem_list['record_date'])\n",
    "\n",
    "\n",
    "# Perform full outer join to include all patients and dates from any source\n",
    "merged_data = pd.merge(merged_data, diagnosis[['empi', 'record_date', 'combined_diagnosis']],\n",
    "                       on=['empi', 'record_date'], how='outer')\n",
    "merged_data = pd.merge(merged_data, medications[['empi', 'record_date', 'medications']],\n",
    "                       on=['empi', 'record_date'], how='outer')\n",
    "merged_data = pd.merge(merged_data, problem_list[['empi', 'record_date', 'combined_diagnosis']],\n",
    "                       on=['empi', 'record_date'], how='outer', suffixes=('', '_problem_list'))\n",
    "merged_data = pd.merge(filtered_notes, dob_data, on='empi', how='outer')\n",
    "# Fill missing filtered notes with explicit 'Null' for clarity\n",
    "merged_data['filtered_text'].fillna('Null', inplace=True)\n",
    "\n",
    "# Calculate age only where dob is available and record_date is not missing\n",
    "merged_data['age'] = merged_data.apply(lambda x: (x['record_date'] - x['dob']).days // 365 if pd.notnull(x['dob']) and pd.notnull(x['record_date']) else None, axis=1)\n",
    "\n",
    "merged_data.to_csv(\"data/final_patient_data_table_intermediate.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff32c49-fce8-491e-bbae-79d61038b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rename columns to match the output table requirements\n",
    "merged_data.rename(columns={\n",
    "    'filtered_text': 'clinical_notes',\n",
    "    'combined_diagnosis': 'diagnoses',\n",
    "    'combined_diagnosis_problem_list': 'problem_lists'\n",
    "}, inplace=True)\n",
    "\n",
    "def aggregate_info(row):\n",
    "    parts = []\n",
    "\n",
    "    age = int(row['age']) if pd.notna(row['age']) else \"unknown\"\n",
    "    \n",
    "    parts.append(f\"The patient was {age} years old. On {row['record_date']}, the following were documented in the EHRs: \")\n",
    "    \n",
    "    if pd.notna(row['diagnoses']):\n",
    "        parts.append(f\"The patient was diagnosed with: {row['diagnoses']}\")\n",
    "        \n",
    "    if pd.notna(row['medications']):\n",
    "        parts.append(f\"The patient received the following medications: {row['medications']}\")\n",
    "        \n",
    "    if pd.notna(row['problem_lists']):\n",
    "        parts.append(f\"The problem list included: {row['problem_lists']}\")\n",
    "\n",
    "    if pd.notna(row['clinical_notes']) and row['clinical_notes'] != \"Null\":\n",
    "        parts.append(f\"The clinical notes documented: {row['clinical_notes']}\")\n",
    "    \n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "merged_data['record_date'] = pd.to_datetime(merged_data['record_date']).dt.date\n",
    "merged_data['medical_summary'] = merged_data.apply(aggregate_info, axis=1)\n",
    "\n",
    "# Reorder columns for the final output table\n",
    "final_table = merged_data[['empi', 'record_date', 'age', 'clinical_notes', \n",
    "                           'diagnoses', 'medications', 'problem_lists', 'medical_summary']]\n",
    "# Save the final table to a new CSV file\n",
    "final_table.to_csv(\"data/final_patient_data_table.csv\", index=False)\n",
    "\n",
    "print(\"Final table created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
