{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from ollama import generate\n",
    "from sentence_transformers import CrossEncoder\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from BioClinicalBERTEmbeddings import BioClinicalBERTEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "hf_model_cache = os.path.join(os.getcwd(), '.hg_model_cache')\n",
    "os.environ['HF_HOME'] = hf_model_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bcf59b1157ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('data/2025_ADRD_case_finding/data/adrd_study_700_notes_all.csv')\n",
    "label = pd.read_csv('data/2025_ADRD_case_finding/data/adrd_study_700_label.csv')\n",
    "diagnosis_list = pd.read_csv(\"data/2025_ADRD_case_finding/data/adrd_combined_diagnosis.csv\")  # Result of Diagnosis query\n",
    "problem_list = pd.read_csv(\"data/2025_ADRD_case_finding/data/adrd_combined_problems.csv\")  # Result of Problem List query\n",
    "\n",
    "\n",
    "has_label = label[label['rand_ind'] <= 33]\n",
    "len(has_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0986e0f41e3b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_list['diagnosis_date'] = pd.to_datetime(problem_list['diagnosis_date'])\n",
    "filter_date = pd.to_datetime('2024-01-31')\n",
    "problem_list = problem_list[problem_list['diagnosis_date'] <= filter_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f18d338921428",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_list['diagnosis_date'] = pd.to_datetime(diagnosis_list['diagnosis_date'])\n",
    "diagnosis_list = diagnosis_list[diagnosis_list['diagnosis_date'] <= filter_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d8f9036ff6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "no_notes = 0\n",
    "no_notes_yes_structured = 0\n",
    "for _,label in has_label.iterrows():\n",
    "    patient_notes = notes[notes['empi'] == label['empi']]\n",
    "    patient_diagnosis = diagnosis_list[diagnosis_list['empi'] == label['empi']]\n",
    "    patient_problem = problem_list[problem_list['empi'] == label['empi']]\n",
    "    if len(patient_notes) > 0:  # some patients have ADRD labels but no notes. Filtering them out\n",
    "        for _, note in patient_notes.iterrows():\n",
    "            content = note['notetxt']\n",
    "            metadata = {\n",
    "                'dob': label['dob'],\n",
    "                'empi': label['empi'],\n",
    "                'notetype': note['notetype'],\n",
    "                'report_date': note['report_date'],\n",
    "                'report_description': note['report_description'],\n",
    "                'report_number': note['report_number'],\n",
    "            }\n",
    "            documents.append(Document(page_content=content, metadata=metadata))\n",
    "    else:\n",
    "        no_notes += 1\n",
    "        if len(patient_problem) > 0 or len(patient_diagnosis) > 0:\n",
    "            no_notes_yes_structured += 1\n",
    "            print(f\"{label['empi']} has no notes but have structured data\" )\n",
    "        else:\n",
    "            print(f\"{label['empi']} has no notes nor any structured data\" )\n",
    "\n",
    "\n",
    "print(\"Documents done\")\n",
    "print(len(documents))\n",
    "print(f\"{no_notes} have no notes\")\n",
    "print(f\"{no_notes_yes_structured} have no notes but have structured data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a345b0f16a57b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "def run_experiment(chunk_strategy: str, embedding_name: str, search_method: str, llm_name: str, result_file: str):\n",
    "    \"\"\"\n",
    "    Reports the confusion matrix, time spent, model settings, and response for the given setting in csv format.\n",
    "\n",
    "    Args:\n",
    "        chunk_strategy (str): \"rule_based\" or \"rn_separators_semantic\"\n",
    "        embedding_name (str): \"mpnet\" or \"bioclinicalbert\" or \"text-embedding-3-large\"\n",
    "        search_method (str): \"Cosine similarity\" or \"Max marginal relevance\"\n",
    "        llm_name (str): \"GPT-4o\" or any ollama model name\n",
    "        result_file (str): The output file in csv format.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    if embedding_name == \"mpnet\":\n",
    "        bi_encoder_model = HuggingFaceEmbeddings(model_name='all-mpnet-base-v2', model_kwargs={'device': 'cuda'}, encode_kwargs={'batch_size': 256, 'normalize_embeddings': False})\n",
    "\n",
    "        if chunk_strategy == \"rule_based\":\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_0to300',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "            vectorstore2 = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_300toEnd',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "        elif chunk_strategy == \"rn_separators_semantic\":\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_0to300_{chunk_strategy}',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "            vectorstore2 = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_300toEnd_{chunk_strategy}',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "        elif chunk_strategy == \"recursive\":\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=f'chroma_mpnet_600_100_0to300_separators',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "            vectorstore2 = Chroma(\n",
    "                persist_directory=f'chroma_mpnet_600_100_300toEnd_separators',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "        else:\n",
    "            print(\"Embedding strategy unknown\")\n",
    "            return\n",
    "    elif embedding_name == \"bioclinicalbert\":\n",
    "        bi_encoder_model = BioClinicalBERTEmbeddings(device=\"cuda\")\n",
    "\n",
    "        if chunk_strategy == \"rule_based\":\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_0to33',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "            vectorstore2 = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_33toEnd',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "        elif chunk_strategy == \"rn_separators_semantic\":\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_0to300',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "            vectorstore2 = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_300toEnd',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "        else:\n",
    "            print(\"Embedding strategy unknown\")\n",
    "            return\n",
    "    elif embedding_name == \"text-embedding-3-large\":\n",
    "        bi_encoder_model = AzureOpenAIEmbeddings(\n",
    "        )\n",
    "\n",
    "        if chunk_strategy == \"rule_based\":\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_0to33',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "            vectorstore2 = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_33toEnd',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "\n",
    "        elif chunk_strategy == \"recursive\":\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_33toEnd_time_measurement_REALLY_final',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "            vectorstore2 = Chroma(\n",
    "                persist_directory=f'chroma_{embedding_name}_{chunk_strategy}_33toEnd_time_measurement_REALLY_final',\n",
    "                embedding_function=bi_encoder_model\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(\"Combination doesn't exist\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Embedding name unknown\")\n",
    "        return\n",
    "\n",
    "    print(f\"Created Chroma vector store with {vectorstore._collection.count()} embeddings.\")\n",
    "    print(f\"Created Chroma vector store with {vectorstore2._collection.count()} embeddings.\")\n",
    "\n",
    "    cross_encoder_model_name = \"cross-encoder/ms-marco-MiniLM-L6-v2\" # Or another Cross-encoder\n",
    "    cross_encoder = CrossEncoder(cross_encoder_model_name)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    records = {\n",
    "        'empi': [],\n",
    "        'prediction': [],\n",
    "        'label': [],\n",
    "        'prompt': [],\n",
    "        'top-k': [],\n",
    "        'response': [],\n",
    "        'result': [],\n",
    "        'chunk_method': [],\n",
    "        'embedding_model': [],\n",
    "        'cross_encoder_model': [],\n",
    "        'search_method': [],\n",
    "        'llm_model': [],\n",
    "        'retrieval_time (s)': [],\n",
    "        'generation_time (s)': [],\n",
    "        'total_time (s)': [],\n",
    "        'system_prompt': [],\n",
    "        'query_phrase': []\n",
    "    }\n",
    "\n",
    "    client = None\n",
    "    deployment = None\n",
    "    if llm_name == \"GPT-4o\":\n",
    "        client = AzureOpenAI(\n",
    "        )\n",
    "        deployment = \"gpt-4o-2\"\n",
    "\n",
    "\n",
    "    for index,label in has_label.iterrows():\n",
    "        patient_notes = notes[notes['empi'] == label['empi']]\n",
    "        # these hard coded patient EMPIs have no patient notes but structured data, include them.\n",
    "        if len(patient_notes) > 0:\n",
    "            query = 'Dementia'\n",
    "            records['query_phrase'].append(query)\n",
    "            target_empi = label['empi']\n",
    "            missing_doc = []\n",
    "            print(f'Patient with EMPI {target_empi}')\n",
    "\n",
    "            start_timer = time.perf_counter()\n",
    "            if search_method == \"Cosine similarity\":\n",
    "                retrieved_docs = vectorstore.similarity_search(query, k=30, filter={\"empi\": target_empi})\n",
    "                if len(retrieved_docs) == 0:\n",
    "                    retrieved_docs = vectorstore2.similarity_search(query, k=30, filter={\"empi\": target_empi})\n",
    "                    if len(retrieved_docs) == 0:\n",
    "                        print(f\"{target_empi} found no notes\")\n",
    "            elif search_method == \"Max marginal relevance\":\n",
    "                retrieved_docs = vectorstore.max_marginal_relevance_search(query, k=30, fetch_k=40, filter={\"empi\": target_empi})\n",
    "                # missing_doc = vectorstore.get(\n",
    "                #     where={\"empi\": target_empi}\n",
    "                # )\n",
    "                # print(missing_doc)\n",
    "                if len(retrieved_docs) == 0:\n",
    "                    retrieved_docs = vectorstore2.max_marginal_relevance_search(query, k=30, fetch_k=40, filter={\"empi\": target_empi})\n",
    "                    # missing_doc = vectorstore2.get(\n",
    "                    #     where={\"empi\": target_empi}\n",
    "                    # )\n",
    "\n",
    "                    if len(retrieved_docs) == 0:\n",
    "                        print(f\"{target_empi} found no notes\")\n",
    "            else:\n",
    "                print(\"Search method unknown\")\n",
    "                return\n",
    "\n",
    "            context = []\n",
    "\n",
    "            # from langchain_community.utils.math import cosine_similarity\n",
    "            #\n",
    "            #\n",
    "            # query_embed = bi_encoder_model.embed_query(query)\n",
    "            # for d in retrieved_docs:\n",
    "            #     d_embed = bi_encoder_model.embed_query(d.page_content)\n",
    "            #     score = cosine_similarity([query_embed], [d_embed])[0][0]\n",
    "            #     print(f\"Relevance score of {score}:\")\n",
    "            #     print(d.page_content)\n",
    "            #     print()\n",
    "            #\n",
    "            # for d in missing_doc['documents']:\n",
    "            #     if chunk_contains in d:\n",
    "            #         d_embed = bi_encoder_model.embed_query(d)\n",
    "            #         score = cosine_similarity([query_embed], [d_embed])[0][0]\n",
    "            #         print(f\"------------------\\nRelevance score of missed doc {score}:\")\n",
    "            #         print(d)\n",
    "            #         print()\n",
    "            #\n",
    "            # return\n",
    "\n",
    "\n",
    "            # Prepare pairs for the Cross-encoder\n",
    "            if len(retrieved_docs) > 0:\n",
    "                cross_encoder_inputs = [[query, doc.page_content] for doc in retrieved_docs]\n",
    "\n",
    "                # Get scores\n",
    "                cross_encoder_scores = cross_encoder.predict(cross_encoder_inputs)\n",
    "\n",
    "                # Combine documents and scores, sort by score descending\n",
    "                scored_docs = sorted(zip(cross_encoder_scores, retrieved_docs), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "                # Select the top N reranked documents\n",
    "                top_n_reranked = 15\n",
    "\n",
    "                final_retrieved_docs = []\n",
    "                for score, doc in scored_docs[:top_n_reranked]:\n",
    "                    final_retrieved_docs.append(doc)\n",
    "\n",
    "                retrieval_done_time = time.perf_counter() - start_timer\n",
    "\n",
    "                records['top-k'].append(top_n_reranked)\n",
    "                records['chunk_method'].append(\"section based\")\n",
    "                if embedding_name == \"text-embedding-3-large\":\n",
    "                    records['embedding_model'].append(embedding_name)\n",
    "                else:\n",
    "                    records['embedding_model'].append(bi_encoder_model.model_name)\n",
    "                records['cross_encoder_model'].append(cross_encoder_model_name if scored_docs else \"None\")\n",
    "                records['search_method'].append(search_method)\n",
    "                records['retrieval_time (s)'].append(retrieval_done_time)\n",
    "\n",
    "                context = \"\\n\\n\\n\".join([\n",
    "                    f\"--- Note Type: {doc.metadata['notetype']}, Report Date: {doc.metadata['report_date']}, Report Description: {doc.metadata.get('report_description')}, Date of Birth: {doc.metadata['dob']}, Report Number: {doc.metadata['report_number']}{f', Section Name: {doc.metadata['section_name']}' if doc.metadata.get('section_name') else ' '} ---\\nEXCERPT START:\\n\\n{doc.page_content}\\n\\nEXCERPT END.\"\n",
    "                    for doc in final_retrieved_docs\n",
    "                ])\n",
    "            else:\n",
    "                retrieval_done_time = time.perf_counter() - start_timer\n",
    "                records['top-k'].append(\"NA\")\n",
    "                records['chunk_method'].append(\"NA\")\n",
    "                records['embedding_model'].append(\"NA\")\n",
    "                records['cross_encoder_model'].append(\"NA\")\n",
    "                records['search_method'].append(\"NA\")\n",
    "                records['retrieval_time (s)'].append(\"NA\")\n",
    "\n",
    "            diagnoses = \"\\n\".join([f\"- {doc['diagnosis_date']}: {doc['combined_diagnosis']}\"\n",
    "                                   for _,doc in diagnosis_list[diagnosis_list['empi'] == target_empi].iterrows()])\n",
    "\n",
    "            problems = \"\\n\".join([f\"- {doc['diagnosis_date']}: {doc['combined_diagnosis']}\"\n",
    "                                   for _,doc in problem_list[problem_list['empi'] == target_empi].iterrows()])\n",
    "\n",
    "            # llm_query = '''\n",
    "            # As of the most recent available documentation, does the patient have dementia?\n",
    "            # Carefully review the timeline of clinical documentation. Determine whether there is an explicit and confirmed diagnosis of dementia specifically for this patient (not a family member or relative).\n",
    "            # Pay close attention to when information was recorded — prioritize more recent notes and diagnoses, but consider earlier entries to evaluate progression over time.\n",
    "            # Disregard:\n",
    "            #     - Mentions of cognitive complaints alone\n",
    "            #     - Medications or treatment trials (e.g., rivastigmine, Aricept) without a confirmed diagnosis\n",
    "            #     - Indirect assessments (e.g., \"work-up for dementia\" or \"referred for evaluation\") unless they result in a clear diagnosis\n",
    "            #\n",
    "            # ONLY START YOUR RESPONSE WITH \\\"YES\\\" or \\\"NO\\\", fall back to NO if unsure. Proceed with explanations AFTER you've given your answer. Example: \\\"Yes, the patient has dementia...\\\" or \\\"No, the patient does not have dementia\\\", but NEVER start with \\\"Based on the provided documentation\\\"... because the response did not immediately start with the answer.\n",
    "            # '''\n",
    "\n",
    "            # llm_query = \"Does the patient have dementia? Determine if there is an explicit and confirmed diagnosis of dementia specifically for this patient (not a family member or relative). Disregard mentions of cognitive complaints, medications, treatment trials (e.g., rivastigmine or Aricept), or indirect assessments (such as work-ups or evaluations) that do not explicitly state a dementia diagnosis. Consider more recent notes with more importance than older ones, and allow newer notes to negate older diagnosis if dementia information is conflicting.\\n\\nIf results of cognitive tests like MMSE, MOCA, or Mini-cog are mentioned, interpret them as following:\\nMMSE: 24 and higher: Normal cognition, no dementia. 19-23: mild dementia. 10-18: moderate dementia. 9 and lower: severe dementia.\\n\\nMOCA: 26 or above: normal. 18-25: mild cognitive impairment. 10-17: moderate cognitive impairment. Less than 10: severe cognitive impairment.\\n\\nMini-cog: If 0 of out 3 words are recalled, it is a positive screen for dementia, regardless of the clock-drawing. If all 3 words are recalled, it is immediately a negative screen for dementia, regardless of the clock-drawing. If the patient recalls 1-2 words, then the clinician needs to further refer to the clock-drawing: If there is 1-2 words (of out 3) for recall, with a normal clock, it is a negative screen for dementia. If there is 1-2 words (of out 3) for recall, with an abnormal clock, it is a positive screen for dementia.\\n\\nONLY START YOUR RESPONSE WITH \\\"YES\\\" or \\\"NO\\\", fall back to NO if unsure. Proceed with explanations AFTER you've given your answer. Example: \\\"Yes, the patient has dementia...\\\" or \\\"No, the patient does not have dementia.\\\"\"\n",
    "\n",
    "            llm_query = \"\"\"Step 1: Analyze the Timeline of Documentation\n",
    "    Review the complete timeline of clinical notes and diagnoses to determine if a diagnosis of dementia was explicitly made for this patient (not for a family member).\n",
    "    Prioritize more recent clinical notes and diagnoses, ensuring they align with and confirm earlier findings. If there is only one dementia-related diagnosis code (e.g., Alzheimer’s disease or related dementia), subsequent documentation must continue to support or confirm the diagnosis for it to be valid.\n",
    "\n",
    "\n",
    "    Step 2: Assess Supporting Evidence\n",
    "    Consider Cognitive Test Scores: Include scores from cognitive assessments (e.g., MoCA, MMSE). Low scores alone do not confirm dementia but, if consistently impaired over time or paired with a diagnosis, they strengthen the case.\n",
    "    Evaluate Functional Status: If the patient is described as functionally independent or improving and there is no corroborating evidence of progressive decline, carefully reassess the validity of the earlier dementia diagnosis.\n",
    "    Transitions in Diagnosis: If earlier records list dementia but newer documentation clarifies mild cognitive impairment (MCI) or indicates improvement, favor the most recent clinically-supported status only if it contradicts earlier findings.\n",
    "\n",
    "\n",
    "    Step 3: Confirm Continuity of Evidence\n",
    "    Explicitly check whether the latest clinical documentation reaffirms an earlier dementia-related diagnosis. If the most recent evidence does not explicitly confirm or support the earlier diagnosis, reassess its validity.\n",
    "\n",
    "\n",
    "    Step 4: Disregard Irrelevant Mentions\n",
    "    Ignore the following unless explicitly tied to a clinical confirmation of dementia:\n",
    "    Memory concerns or subjective complaints without diagnosis.\n",
    "    Medications (e.g., donepezil, rivastigmine) unless paired with a documented diagnosis.\n",
    "    Phrases like “rule out dementia,” “referred for cognitive workup,” or “possible Alzheimer’s” unless subsequent documentation confirms diagnosis.\n",
    "    Isolated billing codes or problem list entries unless supported by clinical notes or functional assessments.\n",
    "\n",
    "\n",
    "    Step 5: Answer Classification\n",
    "    Provide your answer based on the following criteria:\n",
    "\n",
    "    \"Yes\" if:\n",
    "    There is an explicit dementia diagnosis in the most recent clinical documentation and it aligns with or confirms earlier findings.\n",
    "    The diagnosis is supported by clinical observations, cognitive testing, or evidence of progressive functional decline.\n",
    "    \"No\" if:\n",
    "    There is no confirmed dementia diagnosis in the most recent documentation.\n",
    "    The latest evidence indicates MCI or contradicts earlier mentions of dementia.\n",
    "    The earlier dementia diagnosis is unsupported by subsequent clinical findings, particularly when there is only one dementia-related diagnosis code.\"\"\"\n",
    "\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "    Below are some excerpts from clinical notes for this patient\n",
    "\n",
    "    ------------------------------------------\n",
    "\n",
    "    Patient Notes:\n",
    "    {context}\n",
    "\n",
    "    ------------------------------------------\n",
    "\n",
    "\n",
    "    Given information above, answer this question: {llm_query}\n",
    "    \"\"\"\n",
    "\n",
    "            records['prompt'].append(prompt)\n",
    "\n",
    "            system_prompt='''You are a knowledgeable and elite professional healthcare specialist, able to answer complicated and medical related questions from clinical notes. When answering questions, answer based on your knowledge and the provided patient notes. The question will be answered by yes or no format. After answering the question, give thorough explanations and reasons for your decision. Include specific information from the notes and cite the report number when it\\'s helpful. Make your answer evidence based. Start your response with either YES or NO, fall back to NO if unsure. ONLY START YOUR RESPONSE WITH \\\"YES\\\" or \\\"NO\\\", fall back to NO if unsure. Proceed with explanations AFTER you've given your answer. Example: \\\"Yes, the patient has dementia...\\\" or \\\"No, the patient does not have dementia\\\", but NEVER start with \\\"Based on the provided documentation\\\"... because the response did not immediately start with the answer.'''\n",
    "\n",
    "            records['system_prompt'].append(system_prompt)\n",
    "\n",
    "            if llm_name == \"GPT-4o\":\n",
    "                records['llm_model'].append(llm_name)\n",
    "                start_timer = time.perf_counter()\n",
    "                try:\n",
    "                    response = client.chat.completions.create(\n",
    "                        messages=[\n",
    "                            {\n",
    "                                'role': 'system',\n",
    "                                'content':system_prompt\n",
    "                            },\n",
    "                            {\n",
    "                                'role': 'user',\n",
    "                                'content': prompt\n",
    "                            }\n",
    "                        ],\n",
    "                        max_tokens=4096,\n",
    "                        temperature=0,\n",
    "                        model=deployment\n",
    "                    )\n",
    "\n",
    "                    llm_response = response.choices[0].message.content\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"An unexpected error occurred: {e}\")\n",
    "                    llm_response = \"\"\n",
    "\n",
    "                finally:\n",
    "                    generation_done_time = time.perf_counter() - start_timer\n",
    "                    records['generation_time (s)'].append(generation_done_time)\n",
    "                    records['total_time (s)'].append(retrieval_done_time + generation_done_time)\n",
    "\n",
    "            else:\n",
    "                records['llm_model'].append(llm_name)\n",
    "                start_timer = time.perf_counter()\n",
    "                response = generate(model=llm_name,\n",
    "                                    system=system_prompt,\n",
    "                                    prompt=prompt,\n",
    "                                    options={'temperature': 0}\n",
    "                    )\n",
    "                generation_done_time = time.perf_counter() - start_timer\n",
    "                records['generation_time (s)'].append(generation_done_time)\n",
    "                records['total_time (s)'].append(retrieval_done_time + generation_done_time)\n",
    "                llm_response = response.response\n",
    "\n",
    "            records['response'].append(llm_response)\n",
    "            print(f'\\n\\n{llm_response}\\n\\n')\n",
    "\n",
    "            if 'yes' in llm_response.lower()[:10]:\n",
    "                predicted_label = 'yes'\n",
    "            elif 'no' in llm_response.lower()[:10]:\n",
    "                predicted_label = 'no'\n",
    "            else:\n",
    "                predicted_label = 'no'\n",
    "\n",
    "            records['prediction'].append(predicted_label)\n",
    "            actual_label = label['final_label'].lower()\n",
    "\n",
    "            records['label'].append(actual_label)\n",
    "            records['empi'].append(target_empi)\n",
    "\n",
    "            if actual_label == 'yes':\n",
    "                if predicted_label == 'yes':\n",
    "                    tp += 1\n",
    "                    records['result'].append('tp')\n",
    "                else:\n",
    "                    fn += 1\n",
    "                    records['result'].append('fn')\n",
    "            elif actual_label == 'no':\n",
    "                if predicted_label == 'no':\n",
    "                    tn += 1\n",
    "                    records['result'].append('tn')\n",
    "                else:\n",
    "                    fp += 1\n",
    "                    records['result'].append('fp')\n",
    "\n",
    "    print(f'TP = {tp}, FP = {fp}, TN = {tn}, FN = {fn}')\n",
    "\n",
    "    saving = pd.DataFrame(records)\n",
    "    print(f\"Total latency: {saving['total_time (s)'].sum()} seconds. Remember to run reanalysis.ipynb\")\n",
    "    saving.to_csv(result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8408a173d0aa27",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "run_experiment(chunk_strategy=\"rule_based\", embedding_name=\"text-embedding-3-large\", search_method=\"Max marginal relevance\", llm_name=\"GPT-4o\", result_file=\"cost_analysis.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
