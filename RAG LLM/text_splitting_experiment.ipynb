{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "\n",
    "hf_model_cache = os.path.join(os.getcwd(), '.hg_model_cache')\n",
    "os.environ['HF_HOME'] = hf_model_cache\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from BioClinicalBERTEmbeddings import BioClinicalBERTEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79eb15eda47a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('data/2025_ADRD_case_finding/data/adrd_study_700_notes_all.csv')\n",
    "label = pd.read_csv('data/2025_ADRD_case_finding/data/adrd_study_700_label.csv')\n",
    "has_label = label[label['rand_ind'] > 33]\n",
    "# rule_based_chunk = pd.read_csv('rule_based_33toEnd.csv', header = None, names=['report_number', 'section_name', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2967022b7642c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(has_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb4fdeda18621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "969ff9c6ac42029f",
   "metadata": {},
   "source": [
    "# Before Chunking\n",
    "\n",
    "Add each patient's doctor note as a document along with patient's metadata\n",
    "\n",
    "For raw documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8aeaaa0531c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for _,label in has_label.iterrows():\n",
    "    patient_notes = notes[notes['empi'] == label['empi']]\n",
    "    if len(patient_notes) > 0:  # some patients have ADRD labels but no notes. Filtering them out\n",
    "        for _, note in patient_notes.iterrows():\n",
    "            content = note['notetxt']\n",
    "            metadata = {\n",
    "                'dob': label['dob'],\n",
    "                'empi': label['empi'],\n",
    "                'notetype': note['notetype'],\n",
    "                'report_date': note['report_date'],\n",
    "                'report_description': note['report_description'],\n",
    "                'report_number': note['report_number'],\n",
    "            }\n",
    "            documents.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "print(\"Documents done\")\n",
    "print(len(documents))\n",
    "\n",
    "chunk_size = 600\n",
    "chunk_overlap = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f1df86f042e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name='all-mpnet-base-v2', model_kwargs={'device': 'cuda'}, encode_kwargs={'batch_size': 512, 'normalize_embeddings': False})\n",
    "embedding_model = BioClinicalBERTEmbeddings(device='cuda')\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding_model,\n",
    "    persist_directory=\"chroma_bioclinicalbert_rule_based_33toEnd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e5cfae3934092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19e921863b4d5f23",
   "metadata": {},
   "source": [
    "# Chunk using `RecursiveCharacterTextSplitter`\n",
    "\n",
    "Using default separators and print the chunked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc58e45a6bbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "\n",
    "chunks_1 = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "print(f\"{len(documents)} documents split into {len(chunks_1)} chunks.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ed89370ac829f",
   "metadata": {},
   "source": [
    "# Use different separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75e4b94dd217e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    "    separators=[\n",
    "        '\\r\\n\\r\\n\\r\\n',\n",
    "        '\\r\\n\\r\\n',\n",
    "        '\\r\\n',\n",
    "        '.',\n",
    "        ',',\n",
    "        ' ',\n",
    "        ''\n",
    "    ]\n",
    ")\n",
    "start = time.perf_counter()\n",
    "chunks_2 = text_splitter.split_documents(documents)\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(f\"{len(documents)} documents split into {len(chunks_2)} chunks.\\n\")\n",
    "print(f\"Splitting took {end-start} seconds.\")\n",
    "\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name='all-mpnet-base-v2', model_kwargs={'device': 'cuda'}, encode_kwargs={'batch_size': 512, 'normalize_embeddings': False})\n",
    "#\n",
    "# from langchain_chroma import Chroma\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     chunks_2,\n",
    "#     embedding_model,\n",
    "#     persist_directory=f'chroma_mpnet_{chunk_size}_{chunk_overlap}_33toEnd_separators'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a32f91ecaea08",
   "metadata": {},
   "source": [
    "# Semantic Chunker\n",
    "\n",
    "https://python.langchain.com/api_reference/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html#langchain_experimental.text_splitter.SemanticChunker.split_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f2868ad8e74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='all-mpnet-base-v2', model_kwargs={'device': 'cuda'}, encode_kwargs={'batch_size': 256, 'normalize_embeddings': False})\n",
    "\n",
    "text_splitter = SemanticChunker(embeddings=embedding_model, buffer_size=2)\n",
    "\n",
    "chunks_3 = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(documents)} documents split into {len(chunks_3)} chunks. Showing first 3 chunks:\\n\")\n",
    "print(chunks_3[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c3f8f1fe43a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='neuml/pubmedbert-base-embeddings', model_kwargs={'device': 'cuda'}, encode_kwargs={'batch_size': 256, 'normalize_embeddings': False})\n",
    "\n",
    "text_splitter = SemanticChunker(embeddings=embedding_model)\n",
    "\n",
    "chunks_4 = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(documents)} documents split into {len(chunks_4)} chunks. Showing first 3 chunks:\\n\")\n",
    "print(chunks_4[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff3b6fc44e3dd3",
   "metadata": {},
   "source": [
    "# Different sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43a00c1833046b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "class CustomSemanticChunker(SemanticChunker):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def split_text(\n",
    "        self,\n",
    "        text: str,\n",
    "    ) -> List[str]:\n",
    "        # Splitting the essay (by default on '.', '?', and '!')\n",
    "        single_sentences_list = re.split(self.sentence_split_regex, text)\n",
    "\n",
    "        # having len(single_sentences_list) == 1 would cause the following\n",
    "        # np.percentile to fail.\n",
    "        if len(single_sentences_list) == 1:\n",
    "            return single_sentences_list\n",
    "        # similarly, the following np.gradient would fail\n",
    "        if (\n",
    "            self.breakpoint_threshold_type == \"gradient\"\n",
    "            and len(single_sentences_list) == 2\n",
    "        ):\n",
    "            return single_sentences_list\n",
    "        distances, sentences = self._calculate_sentence_distances(single_sentences_list)\n",
    "        if self.number_of_chunks is not None:\n",
    "            breakpoint_distance_threshold = self._threshold_from_clusters(distances)\n",
    "            breakpoint_array = distances\n",
    "        else:\n",
    "            (\n",
    "                breakpoint_distance_threshold,\n",
    "                breakpoint_array,\n",
    "            ) = self._calculate_breakpoint_threshold(distances)\n",
    "\n",
    "        indices_above_thresh = [\n",
    "            i\n",
    "            for i, x in enumerate(breakpoint_array)\n",
    "            if x > breakpoint_distance_threshold\n",
    "        ]\n",
    "\n",
    "        chunks = []\n",
    "        start_index = 0\n",
    "\n",
    "        # Iterate through the breakpoints to slice the sentences\n",
    "        for index in indices_above_thresh:\n",
    "            # The end index is the current breakpoint\n",
    "            end_index = index\n",
    "\n",
    "            # Slice the sentence_dicts from the current start index to the end index\n",
    "            group = sentences[start_index : end_index + 1]\n",
    "            combined_text = \"\\r\\n\".join([d[\"sentence\"] for d in group])\n",
    "            # If specified, merge together small chunks.\n",
    "            if (\n",
    "                self.min_chunk_size is not None\n",
    "                and len(combined_text) < self.min_chunk_size\n",
    "            ):\n",
    "                continue\n",
    "            chunks.append(combined_text)\n",
    "\n",
    "            # Update the start index for the next group\n",
    "            start_index = index + 1\n",
    "\n",
    "        # The last group, if any sentences remain\n",
    "        if start_index < len(sentences):\n",
    "            combined_text = \"\\r\\n\".join([d[\"sentence\"] for d in sentences[start_index:]])\n",
    "            chunks.append(combined_text)\n",
    "        return chunks\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='all-mpnet-base-v2', model_kwargs={'device': 'cuda'}, encode_kwargs={'batch_size': 512, 'normalize_embeddings': False})\n",
    "\n",
    "\n",
    "text_splitter = CustomSemanticChunker(\n",
    "    embeddings=embedding_model,\n",
    "    sentence_split_regex=r\"\\r\\n\",\n",
    "    buffer_size=3,\n",
    ")\n",
    "filtered_docs =[doc for doc in documents if doc.page_content.strip()]\n",
    "print(\"Start splitting now\")\n",
    "start = time.perf_counter()\n",
    "chunks_5 = text_splitter.split_documents(filtered_docs)\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(f\"{len(filtered_docs)} documents split into {len(chunks_5)} chunks.\\n\")\n",
    "print(f\"Splitting took {end-start} seconds.\")\n",
    "# from langchain_chroma import Chroma\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     chunks_5,\n",
    "#     embedding_model,\n",
    "#     persist_directory=f'chroma_text-embedding-3-large_rn_separators_semantic_300toEnd'\n",
    "# )\n",
    "# print(f\"Created Chroma vector store with {vectorstore._collection.count()} embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5d044e2028084",
   "metadata": {},
   "source": [
    "Section based chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1dbeb8b6bb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rule_based_chunk = pd.read_csv('rule_based_33toEnd.csv', header = None, names=['report_number', 'section_name', 'content'])\n",
    "\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Rule based chunks save to ChromaDB\n",
    "documents = []\n",
    "\n",
    "\n",
    "for index, label in has_label.iterrows():\n",
    "    patient_notes = notes[notes['empi'] == label['empi']]\n",
    "    if len(patient_notes) > 0:\n",
    "        print(label['empi'])\n",
    "        for _, note in patient_notes.iterrows():\n",
    "            note_id = note['report_number']\n",
    "            for _, section in rule_based_chunk[rule_based_chunk['report_number'] == note_id].iterrows():\n",
    "                metadata = {\n",
    "                    'dob': label['dob'],\n",
    "                    'empi': label['empi'],\n",
    "                    'notetype': note['notetype'],\n",
    "                    'report_date': note['report_date'],\n",
    "                    'report_description': note['report_description'],\n",
    "                    'report_number': note['report_number'],\n",
    "                    'section_name': section['section_name']\n",
    "                }\n",
    "                documents.append(Document(page_content=section['content'], metadata=metadata))\n",
    "\n",
    "print(f\"Loading documents took {time.perf_counter() - start_time} seconds.\")\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4e0a5c32a4675",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    # api key here\n",
    ")\n",
    "\n",
    "batch_size = 50\n",
    "delay_in_seconds = 1\n",
    "\n",
    "def batch_documents(docs, size):\n",
    "    \"\"\"Yield successive n-sized chunks from a list of documents.\"\"\"\n",
    "    for i in range(0, len(docs), size):\n",
    "        yield docs[i:i + size]\n",
    "\n",
    "document_batches = list(batch_documents(chunks_2, batch_size))\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "total_time = 10095.399431978352\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=document_batches[0],\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=f'chroma_text-embedding-3-large_recursive_33toEnd_time_measurement_final'\n",
    ")\n",
    "\n",
    "total_time += time.perf_counter() - start_time\n",
    "\n",
    "print(\"First batch processed and DB created\")\n",
    "\n",
    "start_index = 1\n",
    "\n",
    "for i, batch in enumerate(document_batches[start_index:], start=start_index):\n",
    "    print(f\"Processing batch {i+1}/{len(document_batches)}...\")\n",
    "    start_time = time.perf_counter()\n",
    "    while True:\n",
    "        try:\n",
    "            vectorstore.add_documents(documents=batch)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    total_time += time.perf_counter() - start_time\n",
    "    print(f\"Total time so far: {total_time}\")\n",
    "    time.sleep(delay_in_seconds)\n",
    "\n",
    "print(f\"Created Chroma vector store with {vectorstore._collection.count()} embeddings.\")\n",
    "print(f\"Total time: {total_time}\")\n",
    "print(f\"Start date: {start_time}\")\n",
    "with open(\"time recursive\", 'w') as f:\n",
    "    f.write(str(total_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65311e30b729bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_label = label[label['rand_ind'] > 33]\n",
    "rule_based_chunk = pd.read_csv('rule_based_33toEnd.csv', header = None, names=['report_number', 'section_name', 'content'])\n",
    "\n",
    "# Rule based chunks save to ChromaDB\n",
    "documents = []\n",
    "\n",
    "for index, label in has_label.iterrows():\n",
    "    patient_notes = notes[notes['empi'] == label['empi']]\n",
    "    if len(patient_notes) > 0:\n",
    "        print(label['empi'])\n",
    "        for _, note in patient_notes.iterrows():\n",
    "            note_id = note['report_number']\n",
    "            for _, section in rule_based_chunk[rule_based_chunk['report_number'] == note_id].iterrows():\n",
    "                metadata = {\n",
    "                    'dob': label['dob'],\n",
    "                    'empi': label['empi'],\n",
    "                    'notetype': note['notetype'],\n",
    "                    'report_date': note['report_date'],\n",
    "                    'report_description': note['report_description'],\n",
    "                    'report_number': note['report_number'],\n",
    "                    'section_name': section['section_name']\n",
    "                }\n",
    "                documents.append(Document(page_content=section['content'], metadata=metadata))\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "chunk_7 = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(documents)} documents split into {len(chunk_7)} chunks. Showing first 5 chunks:\\n\")\n",
    "print(chunk_7[:5])\n",
    "from langchain_chroma import Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunk_7,\n",
    "    embedding_model,\n",
    "    persist_directory=f'chroma_text-embedding-3-large_rule_based_33toEnd'\n",
    ")\n",
    "print(f\"Created Chroma vector store with {vectorstore._collection.count()} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dae35939d135e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
